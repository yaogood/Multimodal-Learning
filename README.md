# Multimodal-Learning
Summary many kinds multimodal learning algorithms, papers, datasets and codes.

## [Visual Question Answering](https://visualqa.org/)

Paper:  
1 [VQA: Visual Question Answering (ICCV 2015)](https://arxiv.org/pdf/1505.00468.pdf)
2 [Balancing and Answering Binary Visual Questions (CVPR 2016)](https://arxiv.org/pdf/1511.05099.pdf)
3 [Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering (CVPR 2017)](https://arxiv.org/pdf/1612.00837.pdf)
4 []

VQA Models: 
1 [LSTM + CNN Model](https://github.com/GT-Vision-Lab/VQA_LSTM_CNN)
2 [Hierarchical Co-Attention Model](https://github.com/jiasenlu/HieCoAttenVQA)

Dataset: 
* [vqa](https://visualqa.org/download.html) 
  
Accuracy:
| Year | Model | Total-acc |
| -- | -- | --|
| 2019 | LSTM+CNN | |
||||


## Timeline
Weekly dealine is as listed below from the week of June 8th to August 22.
Week0: Summary current works and analyse the datasets.
Week1: Setup the evaluation pipeline. Try previous codes and test the datasets.   
Week2-3: Write our own codes under our proposal.
Week4: Debug, test and improve codes.   
Week5-8: Running codes, summary and analyse results. 
Week5~8: Implement one ablation study every week.
Week9-10 Write paper and keep improving experiments' results.


